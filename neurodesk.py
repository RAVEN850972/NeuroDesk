import os
import json
import uuid
import random
import copy
import logging
import asyncio
from pathlib import Path
from typing import Dict, Any, List, Optional
from neurograph.core.logging import get_logger
import time
import pickle

from openai import OpenAI
from neurograph.integration import NeuroGraphEngine
from neurograph.integration.base import IntegrationConfig, ProcessingResponse, ProcessingRequest
from context_aware_learning_adapter import ContextAwareLearningAdapter
from graph_consolidation import GraphConsolidator
from feedback_learning import FeedbackLearner
from learning_quality_analyzer import LearningQualityAnalyzer

# –ò–º–ø–æ—Ä—Ç –Ω–∞—à–µ–≥–æ –Ω–æ–≤–æ–≥–æ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –∫–æ–Ω–≤–µ–π–µ—Ä–∞
from hybrid_query_pipeline import HybridQueryPipeline

# –ö–ª—é—á OpenAI –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è
SESSION_DIR = Path("./sessions")
SESSION_DIR.mkdir(exist_ok=True)

class PipelineTuner:
    """–ì–µ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–π —Ç—é–Ω–µ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∫–æ–Ω–≤–µ–π–µ—Ä–æ–≤."""
    
    def __init__(self, engine: NeuroGraphEngine):
        self.engine = engine
        self.logger = get_logger("PipelineTuner")
        base_config = engine.config.to_dict()
        
        self.best_config = self._safe_copy(base_config)
        self.best_score = 0.0
        self.mutation_rate = 0.1
        self.iterations_without_improvement = 0
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –º—É—Ç–∞—Ü–∏–∏ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –∫–æ–Ω–≤–µ–π–µ—Ä–∞
        self.hybrid_params = {
            "min_confidence_threshold": 0.4,
            "min_response_length": 20,
            "learning_enabled": True
        }
    
    def _safe_copy(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏."""
        safe_config = {}
        for key, value in config.items():
            try:
                pickle.dumps(value)
                safe_config[key] = copy.deepcopy(value)
            except (pickle.PicklingError, TypeError, AttributeError):
                self.logger.warning(f"Skipping non-serializable config key: {key}")
                safe_config[key] = None
        return safe_config
    
    def mutate_hybrid_params(self) -> Dict[str, Any]:
        """–ú—É—Ç–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –∫–æ–Ω–≤–µ–π–µ—Ä–∞."""
        mutated = self.hybrid_params.copy()
        
        if random.random() < self.mutation_rate:
            # –ú—É—Ç–∏—Ä—É–µ–º –ø–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            mutated["min_confidence_threshold"] = max(0.1, min(0.8, 
                self.hybrid_params["min_confidence_threshold"] + random.uniform(-0.1, 0.1)
            ))
        
        if random.random() < self.mutation_rate:
            # –ú—É—Ç–∏—Ä—É–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É –æ—Ç–≤–µ—Ç–∞
            mutated["min_response_length"] = max(10, min(100,
                self.hybrid_params["min_response_length"] + random.randint(-5, 5)
            ))
        
        return mutated
    
    def evaluate(self, response: ProcessingResponse, feedback: Optional[int] = None) -> float:
        """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–∞."""
        if not response.success:
            return 0.0
        
        score = response.confidence or 0.0
        
        # –ë–æ–Ω—É—Å –∑–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π
        if response.metadata.get("source") == "graph":
            score += 0.2
        elif response.metadata.get("source") == "openai_fallback":
            # OpenAI –ø–æ–ª—É—á–∞–µ—Ç –±–∞–∑–æ–≤—ã–π –±–∞–ª–ª, –Ω–æ –º–µ–Ω—å—à–µ —á–µ–º –≥—Ä–∞—Ñ
            score += 0.1
        
        # –£—á–∏—Ç—ã–≤–∞–µ–º –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        if feedback is not None:
            score += feedback * 0.3
        
        # –£—á–∏—Ç—ã–≤–∞–µ–º —É—Å–ø–µ—à–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
        if response.metadata.get("graph_learning"):
            score += 0.1
        
        return min(1.0, max(0.0, score))
    
    def update(self, config: Dict[str, Any], score: float):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ª—É—á—à–µ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏."""
        if score > self.best_score:
            self.best_score = score
            self.best_config = self._safe_copy(config)
            self.iterations_without_improvement = 0
            self.logger.info(f"–ù–æ–≤–∞—è –ª—É—á—à–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: score={score:.3f}")
        else:
            self.iterations_without_improvement += 1
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –º—É—Ç–∞—Ü–∏—è
        if self.iterations_without_improvement > 10:
            self.mutation_rate = min(0.5, self.mutation_rate * 1.1)
        elif self.iterations_without_improvement == 0:
            self.mutation_rate = max(0.05, self.mutation_rate * 0.9)

class NeuroDesk:
    """–ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–π AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Å –≥–∏–±—Ä–∏–¥–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π."""

    def __init__(self, user_id: str = "anon", config_path: str = None, 
                 openai_api_key: str = None):
        self.user_id = user_id
        self.session_id = f"{user_id}_{int(time.time())}"
        self.logger = get_logger("NeuroDesk")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        self.config = self._load_config(config_path)
        self.engine = NeuroGraphEngine()
        self.engine.initialize(self.config)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –∫–æ–Ω–≤–µ–π–µ—Ä–∞
        self.hybrid_pipeline = HybridQueryPipeline(openai_api_key)
        
        # –¢—é–Ω–µ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        self.tuner = PipelineTuner(self.engine)
        
        # –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self._register_components()
        
        # –°–æ—Å—Ç–æ—è–Ω–∏–µ —Å–µ—Å—Å–∏–∏
        self._load_session_state()
        self._start_background_tasks()
        
        self.logger.info(f"NeuroDesk –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")
    
    def _register_components(self):
        """–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤."""
        provider = self.engine.provider
        
        provider.register_lazy_component(
            "context_aware_learning", 
            lambda: ContextAwareLearningAdapter()
        )
        provider.register_lazy_component(
            "graph_consolidator", 
            lambda: GraphConsolidator()
        )
        provider.register_lazy_component(
            "feedback_learner", 
            lambda: FeedbackLearner()
        )
        provider.register_lazy_component(
            "learning_quality_analyzer", 
            lambda: LearningQualityAnalyzer()
        )
        
        # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –≥–∏–±—Ä–∏–¥–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –∫–∞–∫ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
        provider.register_component("hybrid_pipeline", self.hybrid_pipeline)
    
    def _load_config(self, config_path: str = None) -> IntegrationConfig:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏."""
        from neurograph.integration.config import IntegrationConfigManager
        manager = IntegrationConfigManager()

        if config_path and os.path.exists(config_path):
            return manager.load_config(config_path)
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        config_data = manager.create_template_config("default")
        config_data["components"]["memory"]["params"] = {
            "stm_capacity": 100,
            "ltm_capacity": 5000
        }
        config_data["components"]["semgraph"] = {
            "type": "persistent",
            "params": {
                "file_path": f"graph_{self.user_id}.json",
                "auto_save_interval": 300.0
            }
        }
        return IntegrationConfig(**config_data)
    
    def _load_session_state(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–µ—Å—Å–∏–∏."""
        session_file = SESSION_DIR / f"{self.user_id}_session.json"
        if session_file.exists():
            try:
                with open(session_file, "r", encoding="utf-8") as f:
                    session_data = json.load(f)
                    self.session_id = session_data.get("session_id", self.session_id)
                    self.logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–∞ —Å–µ—Å—Å–∏—è: {self.session_id}")
            except Exception as e:
                self.logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å–µ—Å—Å–∏—é: {e}")
    
    def _save_session_state(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–µ—Å—Å–∏–∏."""
        try:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º ID —Å–µ—Å—Å–∏–∏
            session_file = SESSION_DIR / f"{self.user_id}_session.json"
            with open(session_file, "w", encoding="utf-8") as f:
                json.dump({"session_id": self.session_id}, f)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
            self.logger.info("üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã")
            for comp_name in ["memory", "semgraph"]:
                if self.engine.provider.is_component_available(comp_name):
                    comp = self.engine.provider.get_component(comp_name)
                    if hasattr(comp, "save"):
                        try:
                            comp.save()
                            self.logger.debug(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω –∫–æ–º–ø–æ–Ω–µ–Ω—Ç: {comp_name}")
                        except Exception as e:
                            self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è {comp_name}: {e}")
        
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è: {e}")
    
    def _start_background_tasks(self):
        """–ó–∞–ø—É—Å–∫ —Ñ–æ–Ω–æ–≤—ã—Ö –∑–∞–¥–∞—á."""
        try:
            loop = asyncio.get_event_loop()
            if not loop.is_running():
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º —Ñ–æ–Ω–æ–≤—É—é –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—é
            asyncio.create_task(self._background_consolidation())
        except Exception as e:
            self.logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å —Ñ–æ–Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏: {e}")
    
    async def _background_consolidation(self):
        """–§–æ–Ω–æ–≤–∞—è –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è –≥—Ä–∞—Ñ–∞."""
        while True:
            try:
                await asyncio.sleep(3600)  # –ö–∞–∂–¥—ã–π —á–∞—Å
                
                if (self.engine.provider.is_component_available("graph_consolidator") and 
                    self.engine.provider.is_component_available("semgraph")):
                    
                    consolidator = self.engine.provider.get_component("graph_consolidator")
                    semgraph = self.engine.provider.get_component("semgraph")
                    metrics = consolidator.consolidate(semgraph)
                    self.logger.info(f"üîÑ –§–æ–Ω–æ–≤–∞—è –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è: {metrics}")
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ —Ñ–æ–Ω–æ–≤–æ–π –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏: {e}")
    
    def ask(self, prompt: str, feedback: Optional[int] = None, 
            domain: Optional[str] = None, **kwargs) -> Dict[str, Any]:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –¥–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ —Å –≥–∏–±—Ä–∏–¥–Ω–æ–π –ª–æ–≥–∏–∫–æ–π.
        
        Args:
            prompt: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            feedback: –û–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å (-1, 0, 1)
            domain: –ü—Ä–µ–¥–º–µ—Ç–Ω–∞—è –æ–±–ª–∞—Å—Ç—å
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –æ—Ç–≤–µ—Ç–æ–º –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        try:
            # –ú—É—Ç–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ç—é–Ω–∏–Ω–≥–∞
            mutated_params = self.tuner.mutate_hybrid_params()
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –∫–æ–Ω–≤–µ–π–µ—Ä–∞
            self.hybrid_pipeline.min_confidence_threshold = mutated_params["min_confidence_threshold"]
            self.hybrid_pipeline.min_response_length = mutated_params["min_response_length"]
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å
            request = ProcessingRequest(
                content=prompt,
                request_type="hybrid_query",
                session_id=self.session_id,
                context={
                    "user_id": self.user_id,
                    "domain": domain or "general",
                    "user_context": self._get_user_context()
                },
                explanation_level="detailed",
                **kwargs
            )
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ –≥–∏–±—Ä–∏–¥–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä
            response = self.hybrid_pipeline.process(request, self.engine.provider)
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å, –µ—Å–ª–∏ –µ—Å—Ç—å
            if feedback is not None:
                self._apply_feedback(response, feedback)
            
            # –û—Ü–µ–Ω–∏–≤–∞–µ–º –∏ –æ–±–Ω–æ–≤–ª—è–µ–º —Ç—é–Ω–µ—Ä
            score = self.tuner.evaluate(response, feedback)
            self.tuner.update(mutated_params, score)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            self._save_session_state()
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            result = {
                "answer": response.primary_response,
                "success": response.success,
                "confidence": response.confidence,
                "source": response.metadata.get("source", "unknown"),
                "processing_time": response.processing_time,
                "components_used": response.components_used,
                "explanation": response.explanation,
                "warnings": response.warnings
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞
            if response.metadata.get("source") == "openai_fallback":
                result["learned_from_openai"] = response.metadata.get("graph_learning", False)
                result["tokens_used"] = response.metadata.get("tokens_used")
            
            return result
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –≤ ask(): {e}", exc_info=True)
            return {
                "answer": f"‚ö†Ô∏è –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞: {str(e)}",
                "success": False,
                "confidence": 0.0,
                "source": "error",
                "error": str(e)
            }
    
    def learn(self, text: str, domain: str = "general", 
              context: Dict[str, Any] = None, **kwargs) -> Dict[str, Any]:
        """
        –Ø–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã.
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è
            domain: –ü—Ä–µ–¥–º–µ—Ç–Ω–∞—è –æ–±–ª–∞—Å—Ç—å
            context: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—É—á–µ–Ω–∏—è
        """
        try:
            learning_context = {
                "domain": domain,
                "user_id": self.user_id,
                "learning_type": "explicit",
                **(context or {})
            }
            
            response = self.engine.learn(
                text,
                session_id=self.session_id,
                context=learning_context,
                request_type="context_aware_learning",
                **kwargs
            )
            
            self._save_session_state()
            
            return {
                "success": response.success,
                "message": response.primary_response,
                "nodes_added": response.structured_data.get("learning", {}).get("nodes_added", 0),
                "edges_added": response.structured_data.get("learning", {}).get("edges_added", 0),
                "processing_time": response.processing_time,
                "error": response.error_message if not response.success else None
            }
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –≤ learn(): {e}", exc_info=True)
            return {
                "success": False,
                "message": f"–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {str(e)}",
                "error": str(e)
            }
    
    def _get_user_context(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."""
        context = {
            "user_id": self.user_id,
            "session_id": self.session_id,
            "expertise_areas": [],
            "preferences": {}
        }
        
        # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø–∞–º—è—Ç–∏
        try:
            if self.engine.provider.is_component_available("memory"):
                memory = self.engine.provider.get_component("memory")
                recent_items = memory.get_recent_items(hours=24.0)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–±–ª–∞—Å—Ç–∏ —ç–∫—Å–ø–µ—Ä—Ç–∏–∑—ã –∏–∑ –Ω–µ–¥–∞–≤–Ω–µ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
                domains = set()
                for item in recent_items[:20]:  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 20 —ç–ª–µ–º–µ–Ω—Ç–æ–≤
                    item_domain = item.metadata.get("domain")
                    if item_domain and item_domain != "general":
                        domains.add(item_domain)
                
                context["expertise_areas"] = list(domains)[:5]  # –¢–æ–ø-5
                
        except Exception as e:
            self.logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {e}")
        
        return context
    
    def _apply_feedback(self, response: ProcessingResponse, feedback: int):
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."""
        try:
            if (self.engine.provider.is_component_available("feedback_learner") and 
                self.engine.provider.is_component_available("semgraph")):
                
                feedback_learner = self.engine.provider.get_component("feedback_learner")
                semgraph = self.engine.provider.get_component("semgraph")
                
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ñ–∏–¥–±–µ–∫ –∫ –¥–∏–∞–ø–∞–∑–æ–Ω—É [-1, 1]
                normalized_feedback = max(-1.0, min(1.0, feedback / 5.0))
                
                metrics = feedback_learner.apply_feedback(semgraph, response, normalized_feedback)
                self.logger.info(f"üîÑ –ü—Ä–∏–º–µ–Ω–µ–Ω —Ñ–∏–¥–±–µ–∫ {feedback}: {metrics}")
                
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Ñ–∏–¥–±–µ–∫–∞: {e}")
    
    def get_system_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å–∏—Å—Ç–µ–º—ã."""
        stats = {
            "user_id": self.user_id,
            "session_id": self.session_id,
            "uptime": time.time(),
            "components_status": {},
            "pipeline_stats": {},
            "learning_quality": {},
            "tuner_stats": {
                "best_score": self.tuner.best_score,
                "mutation_rate": self.tuner.mutation_rate,
                "iterations_without_improvement": self.tuner.iterations_without_improvement
            }
        }
        
        # –°—Ç–∞—Ç—É—Å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        for comp_name in ["memory", "semgraph", "nlp", "feedback_learner"]:
            if self.engine.provider.is_component_available(comp_name):
                try:
                    comp = self.engine.provider.get_component(comp_name)
                    if hasattr(comp, "get_statistics"):
                        stats["components_status"][comp_name] = comp.get_statistics()
                    else:
                        stats["components_status"][comp_name] = {"status": "available"}
                except Exception as e:
                    stats["components_status"][comp_name] = {"status": "error", "error": str(e)}
            else:
                stats["components_status"][comp_name] = {"status": "unavailable"}
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –∫–æ–Ω–≤–µ–π–µ—Ä–∞
        try:
            stats["pipeline_stats"] = self.hybrid_pipeline.get_pipeline_stats()
        except Exception as e:
            stats["pipeline_stats"] = {"error": str(e)}
        
        # –ö–∞—á–µ—Å—Ç–≤–æ –æ–±—É—á–µ–Ω–∏—è
        try:
            if self.engine.provider.is_component_available("learning_quality_analyzer"):
                analyzer = self.engine.provider.get_component("learning_quality_analyzer")
                stats["learning_quality"] = analyzer.get_quality_report()
        except Exception as e:
            stats["learning_quality"] = {"error": str(e)}
        
        return stats
    
    def get_learning_quality_report(self, max_entries: int = 10) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ –∫–∞—á–µ—Å—Ç–≤–µ –æ–±—É—á–µ–Ω–∏—è."""
        try:
            if self.engine.provider.is_component_available("learning_quality_analyzer"):
                analyzer = self.engine.provider.get_component("learning_quality_analyzer")
                return analyzer.get_quality_report(max_entries)
            return {"message": "–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∫–∞—á–µ—Å—Ç–≤–∞ –æ–±—É—á–µ–Ω–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"}
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞: {e}", exc_info=True)
            return {"error": str(e)}
    
    def reset_session(self) -> bool:
        """–°–±—Ä–æ—Å —Ç–µ–∫—É—â–µ–π —Å–µ—Å—Å–∏–∏."""
        try:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            self._save_session_state()
            
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é —Å–µ—Å—Å–∏—é
            self.session_id = f"{self.user_id}_{int(time.time())}"
            
            # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–æ–Ω–≤–µ–π–µ—Ä–∞
            self.hybrid_pipeline.stats = {
                "graph_responses": 0,
                "openai_fallbacks": 0,
                "learning_sessions": 0,
                "total_queries": 0
            }
            
            # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Ç—é–Ω–µ—Ä
            self.tuner.best_score = 0.0
            self.tuner.iterations_without_improvement = 0
            self.tuner.mutation_rate = 0.1
            
            self.logger.info(f"–°–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è —Å–µ—Å—Å–∏—è: {self.session_id}")
            return True
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ —Å–±—Ä–æ—Å–∞ —Å–µ—Å—Å–∏–∏: {e}")
            return False
    
    def configure_hybrid_pipeline(self, min_confidence: float = None, 
                                 min_response_length: int = None) -> bool:
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –∫–æ–Ω–≤–µ–π–µ—Ä–∞."""
        try:
            if min_confidence is not None:
                self.hybrid_pipeline.min_confidence_threshold = max(0.1, min(0.9, min_confidence))
            
            if min_response_length is not None:
                self.hybrid_pipeline.min_response_length = max(5, min(200, min_response_length))
            
            self.logger.info(f"–û–±–Ω–æ–≤–ª–µ–Ω—ã –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–æ–Ω–≤–µ–π–µ—Ä–∞: "
                           f"confidence={self.hybrid_pipeline.min_confidence_threshold}, "
                           f"length={self.hybrid_pipeline.min_response_length}")
            return True
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–æ–Ω–≤–µ–π–µ—Ä–∞: {e}")
            return False
    
    def shutdown(self):
        """–ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã."""
        try:
            self.logger.info("üö™ –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã NeuroDesk...")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            self._save_session_state()
            
            # –ó–∞–≤–µ—Ä—à–∞–µ–º –¥–≤–∏–∂–æ–∫
            self.engine.shutdown()
            
            # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            final_stats = self.get_system_stats()
            pipeline_stats = final_stats.get("pipeline_stats", {})
            
            if pipeline_stats:
                self.logger.info(
                    f"üìä –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: "
                    f"–≥—Ä–∞—Ñ={pipeline_stats.get('graph_responses', 0)}, "
                    f"OpenAI={pipeline_stats.get('openai_fallbacks', 0)}, "
                    f"–æ–±—É—á–µ–Ω–∏–µ={pipeline_stats.get('learning_sessions', 0)}"
                )
            
            self.logger.info("‚úÖ NeuroDesk –∑–∞–≤–µ—Ä—à–µ–Ω –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏: {e}")

# –£–¥–æ–±–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤
def create_neurodesk(user_id: str = "default", 
                    config_path: str = None,
                    openai_api_key: str = None) -> NeuroDesk:
    """
    –°–æ–∑–¥–∞–Ω–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ NeuroDesk —Å –≥–∏–±—Ä–∏–¥–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π.
    
    Args:
        user_id: –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        config_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        openai_api_key: API –∫–ª—é—á OpenAI (–∏–ª–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è OPENAI_API_KEY)
    
    Returns:
        –ù–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä NeuroDesk
    """
    return NeuroDesk(user_id=user_id, config_path=config_path, openai_api_key=openai_api_key)

def create_lightweight_neurodesk(user_id: str = "default", 
                                openai_api_key: str = None) -> NeuroDesk:
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –ª–µ–≥–∫–æ–≤–µ—Å–Ω–æ–≥–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ NeuroDesk.
    
    Args:
        user_id: –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è  
        openai_api_key: API –∫–ª—é—á OpenAI
    
    Returns:
        –õ–µ–≥–∫–æ–≤–µ—Å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä NeuroDesk
    """
    # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é –ª–µ–≥–∫–æ–≤–µ—Å–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    return NeuroDesk(user_id=user_id, openai_api_key=openai_api_key)